# -*- coding: utf-8 -*-
"""dog_Finder.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ro1znxg0hupD7A_3kgyAVSVtck7rsZdc
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("jessicali9530/stanford-dogs-dataset")

print("Path to dataset files:", path)

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import image_dataset_from_directory

# 1. Define the root directory where the dataset was downloaded.
# The path should be the one printed by the first cell.
path = '/root/.cache/kagglehub/datasets/jessicali9530/stanford-dogs-dataset/versions/2'
images_dir = os.path.join(path, 'images', 'Images')


# 2. Use image_dataset_from_directory to load the images
img_height, img_width = 128, 128
batch_size = 32

# Create a list of all image file paths by traversing the directory structure.
filepaths = []
labels = []
for breed_folder in os.listdir(images_dir):
    breed_path = os.path.join(images_dir, breed_folder)
    if os.path.isdir(breed_path):
        # Extract the breed label for each image from its directory path.
        label = breed_folder.split('-')[-1] # Assuming the label is the last part after splitting by '-'
        for image_file in os.listdir(breed_path):
            if image_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                filepaths.append(os.path.join(breed_path, image_file))
                labels.append(label)

# Create a pandas DataFrame with two columns: 'filepath' and 'label'
df = pd.DataFrame({'filepath': filepaths, 'label': labels})

# Split the DataFrame into training and validation sets
train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])


# Create TensorFlow datasets from the dataframes
train_ds = image_dataset_from_directory(
    images_dir,
    labels='inferred',
    label_mode='categorical',
    image_size=(img_height, img_width),
    interpolation='nearest',
    batch_size=batch_size,
    shuffle=True,
    seed=42
)

val_ds = image_dataset_from_directory(
    images_dir,
    labels='inferred',
    label_mode='categorical',
    image_size=(img_height, img_width),
    interpolation='nearest',
    batch_size=batch_size,
    shuffle=False,
    seed=42
)


print("Training set size:", len(train_df))
print("Validation set size:", len(val_df))
display(train_df.head())
display(val_df.head())

import os

# Define the root directory where the dataset was downloaded (from the first cell output)
dataset_root = '/root/.kaggle/datasets/jessicali9530/stanford-dogs-dataset'

# List the contents of the downloaded dataset directory
!ls -R {dataset_root}

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.applications import VGG16

# 2. Define the input shape
img_height, img_width = 128, 128
input_shape = (img_height, img_width, 3)
num_classes = 120 # Correcting the number of classes to match the dataset


# 3. Create a sequential model with a pre-trained base
base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)

# Freeze the base model layers
for layer in base_model.layers:
    layer.trainable = False

model = Sequential([
    Input(shape=input_shape),
    base_model,
    Flatten(),
    Dense(512, activation='relu'),
    Dropout(0.5),
    # 4. Add the final dense layer for classification
    Dense(num_classes, activation='softmax')
])

# 5. Compile the model
model.compile(optimizer=Adam(),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 6. Print a summary of the model architecture
model.summary()

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam # Import Adam optimizer to adjust learning rate


# 4. Create data generators for the training and validation sets
batch_size = 32

# Use the TensorFlow datasets directly
train_generator = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
val_generator = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

# 5. Train the compiled model
epochs = 20 # Increased number of epochs
learning_rate = 0.0001 # Reduced learning rate

# Recompile the model with the new learning rate
model.compile(optimizer=Adam(learning_rate=learning_rate),
              loss='categorical_crossentropy',
              metrics=['accuracy'])


history = model.fit(
    train_generator,
    epochs=epochs,
    validation_data=val_generator
)

# 1. Evaluate the model on the validation set
loss, accuracy = model.evaluate(val_generator)

print(f"Validation Loss: {loss:.4f}")
print(f"Validation Accuracy: {accuracy:.4f}")

import matplotlib.pyplot as plt

# Plot training and validation accuracy
plt.figure(figsize=(10, 5))
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# Plot training and validation loss
plt.figure(figsize=(10, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()